{"cells":[{"cell_type":"markdown","metadata":{"id":"py85-Cna3C-m"},"source":["## HW2\n","\n","> using model from [git](https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/senet.py)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BN8Z-T4FV6xV","executionInfo":{"status":"ok","timestamp":1649352217468,"user_tz":-480,"elapsed":21997,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}},"outputId":"e19022e8-06f9-4fe3-ace7-185f301a381c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"f7BIE8HEAA0m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"376ca996-012c-4585-82dd-a81aa4fb1f8f","executionInfo":{"status":"ok","timestamp":1649352301903,"user_tz":-480,"elapsed":6110,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["95f99430742048179038a67ae16aa8ef","a6ad0fb4b51c4b0a920f0b87f6c64994","43a7e279c4bf4157bbd7ea02f175e32d","3f76ce15e5e94099a43be709496bf0b4","7eee45cf04c54715ab15d2880890e1d3","9beefc82a2434ef0afb808bdcac558a2","61a6b7f8c61e4f629f39d431ccb403f5","b4a3088e21964c3983175488618af6ed","4f96dc32239a4a48aef129770c4d56aa","70b3241700ad4c788c1b6cab40340dcc","452f457cc4c34285aef6c6f5620cd0df"]},"id":"DE237Uh3ABKY","outputId":"6d10d1c0-ac5d-42da-86ed-2ff2cee5598e","executionInfo":{"status":"ok","timestamp":1649353205021,"user_tz":-480,"elapsed":7191,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/169001437 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f99430742048179038a67ae16aa8ef"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}],"source":["# see more data augmentation https://pytorch.org/vision/stable/transforms.html\n","# mean = (0.5071, 0.4867, 0.4408)\n","# std = (0.2675, 0.2565, 0.2761)\n","\n","mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n","std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n","\n","# train_transform = transforms.Compose(\n","#     [transforms.RandomHorizontalFlip(p=0.5),\n","#      transforms.ToTensor(),\n","#      transforms.Normalize(mean, std)]) # calculte yourself\n","\n","train_transform = transforms.Compose([\n","    #transforms.ToPILImage(),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","test_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize(mean, std)]) # calculte yourself\n","\n","batch_size = 128\n","num_classes = 100    # check\n","\n","trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n","                                        download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=4)\n","\n","testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n","                                       download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=4)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_PDC78MUALYM","executionInfo":{"status":"ok","timestamp":1649353206899,"user_tz":-480,"elapsed":265,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[],"source":["class BasicResidualSEBlock(nn.Module):\n","\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride, r=16):\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_channels, out_channels * self.expansion, 3, padding=1),\n","            nn.BatchNorm2d(out_channels * self.expansion),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels * self.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * self.expansion, 1, stride=stride),\n","                nn.BatchNorm2d(out_channels * self.expansion)\n","            )\n","\n","        self.squeeze = nn.AdaptiveAvgPool2d(1)\n","        self.excitation = nn.Sequential(\n","            nn.Linear(out_channels * self.expansion, out_channels * self.expansion // r),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(out_channels * self.expansion // r, out_channels * self.expansion),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        shortcut = self.shortcut(x)\n","        residual = self.residual(x)\n","\n","        squeeze = self.squeeze(residual)\n","        squeeze = squeeze.view(squeeze.size(0), -1)\n","        excitation = self.excitation(squeeze)\n","        excitation = excitation.view(residual.size(0), residual.size(1), 1, 1)\n","\n","        x = residual * excitation.expand_as(residual) + shortcut\n","\n","        return F.relu(x)\n","\n","class BottleneckResidualSEBlock(nn.Module):\n","\n","    expansion = 4\n","\n","    def __init__(self, in_channels, out_channels, stride, r=16):\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_channels, out_channels, 3, stride=stride, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_channels, out_channels * self.expansion, 1),\n","            nn.BatchNorm2d(out_channels * self.expansion),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.squeeze = nn.AdaptiveAvgPool2d(1)\n","        self.excitation = nn.Sequential(\n","            nn.Linear(out_channels * self.expansion, out_channels * self.expansion // r),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(out_channels * self.expansion // r, out_channels * self.expansion),\n","            nn.Sigmoid()\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels * self.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * self.expansion, 1, stride=stride),\n","                nn.BatchNorm2d(out_channels * self.expansion)\n","            )\n","\n","    def forward(self, x):\n","\n","        shortcut = self.shortcut(x)\n","\n","        residual = self.residual(x)\n","        squeeze = self.squeeze(residual)\n","        squeeze = squeeze.view(squeeze.size(0), -1)\n","        excitation = self.excitation(squeeze)\n","        excitation = excitation.view(residual.size(0), residual.size(1), 1, 1)\n","\n","        x = residual * excitation.expand_as(residual) + shortcut\n","\n","        return F.relu(x)\n","\n","class SEResNet(nn.Module):\n","\n","    def __init__(self, block, block_num, class_num=100):\n","        super().__init__()\n","\n","        self.in_channels = 64\n","\n","        self.pre = nn.Sequential(\n","            nn.Conv2d(3, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.stage1 = self._make_stage(block, block_num[0], 64, 1)\n","        self.stage2 = self._make_stage(block, block_num[1], 128, 2)\n","        self.stage3 = self._make_stage(block, block_num[2], 256, 2)\n","        self.stage4 = self._make_stage(block, block_num[3], 512, 2)\n","\n","        self.linear = nn.Linear(self.in_channels, class_num)\n","\n","    def forward(self, x):\n","        x = self.pre(x)\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","\n","        x = F.adaptive_avg_pool2d(x, 1)\n","        x = x.view(x.size(0), -1)\n","\n","        x = self.linear(x)\n","\n","        return x\n","\n","\n","    def _make_stage(self, block, num, out_channels, stride):\n","\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels * block.expansion\n","\n","        while num - 1:\n","            layers.append(block(self.in_channels, out_channels, 1))\n","            num -= 1\n","\n","        return nn.Sequential(*layers)\n","\n","def seresnet18():\n","    return SEResNet(BasicResidualSEBlock, [2, 2, 2, 2])\n","\n","def seresnet34():\n","    return SEResNet(BasicResidualSEBlock, [3, 4, 6, 3])\n","\n","def seresnet50():\n","    return SEResNet(BottleneckResidualSEBlock, [3, 4, 6, 3])\n","\n","def seresnet101():\n","    return SEResNet(BottleneckResidualSEBlock, [3, 4, 23, 3])\n","\n","def seresnet152():\n","    return SEResNet(BottleneckResidualSEBlock, [3, 8, 36, 3])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzOoI9FCAiz3","outputId":"f8c3be8a-c481-419d-804e-f831b7f18669","executionInfo":{"status":"ok","timestamp":1649353228152,"user_tz":-480,"elapsed":10050,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SEResNet(\n","  (pre): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (stage1): Sequential(\n","    (0): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=64, out_features=4, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=4, out_features=64, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (1): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=64, out_features=4, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=4, out_features=64, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (2): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=64, out_features=4, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=4, out_features=64, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","  )\n","  (stage2): Sequential(\n","    (0): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=128, out_features=8, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=8, out_features=128, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (1): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=128, out_features=8, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=8, out_features=128, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (2): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=128, out_features=8, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=8, out_features=128, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (3): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=128, out_features=8, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=8, out_features=128, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","  )\n","  (stage3): Sequential(\n","    (0): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (1): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (2): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (3): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (4): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (5): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=256, out_features=16, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=16, out_features=256, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","  )\n","  (stage4): Sequential(\n","    (0): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=512, out_features=32, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=32, out_features=512, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (1): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=512, out_features=32, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=32, out_features=512, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (2): BasicResidualSEBlock(\n","      (residual): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","      )\n","      (shortcut): Sequential()\n","      (squeeze): AdaptiveAvgPool2d(output_size=1)\n","      (excitation): Sequential(\n","        (0): Linear(in_features=512, out_features=32, bias=True)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=32, out_features=512, bias=True)\n","        (3): Sigmoid()\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=100, bias=True)\n",")"]},"metadata":{},"execution_count":5}],"source":["# pick one\n","\n","# 1. model defined by yourself\n","model = seresnet34()        \n","   \n","# 2. off-the-shelf model\n","# see https://pytorch.org/vision/stable/models.html\n","# nn.Linear https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n","# model = models.resnet50(pretrained=True) \n","# model.fc = torch.nn.Linear(2048, num_classes)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yxZ41jR1BT28","executionInfo":{"status":"ok","timestamp":1649353240366,"user_tz":-480,"elapsed":233,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n","train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60,120,160], gamma=0.2)"]},{"cell_type":"code","source":["total_epoch = 200\n","print_per_iteration = 100\n","save_path = '/content/drive/MyDrive/Colab Notebooks/ML_HW2/SEResNet/mimic_resnet.pth'\n","\n","for epoch in range(total_epoch):  # loop over the dataset multiple times\n","    if epoch > 1:\n","      train_scheduler.step(epoch)\n","\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        # print(inputs)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        if (i+1) % print_per_iteration == 0:    # print every 2000 mini-batches\n","            print(f'[ep {epoch + 1}][{i + 1:5d}/{len(trainloader):5d}] loss: {loss.item():.3f}')\n","    torch.save(model, save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LTzbiubhyf2","outputId":"a9aa6433-5bb6-406d-aa1d-503f68a0935e","executionInfo":{"status":"ok","timestamp":1649367039524,"user_tz":-480,"elapsed":13789677,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["[ep 1][  100/  391] loss: 4.391\n","[ep 1][  200/  391] loss: 4.142\n","[ep 1][  300/  391] loss: 3.937\n","[ep 2][  100/  391] loss: 3.672\n","[ep 2][  200/  391] loss: 3.223\n","[ep 2][  300/  391] loss: 3.306\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["[ep 3][  100/  391] loss: 3.240\n","[ep 3][  200/  391] loss: 3.179\n","[ep 3][  300/  391] loss: 3.022\n","[ep 4][  100/  391] loss: 2.613\n","[ep 4][  200/  391] loss: 2.543\n","[ep 4][  300/  391] loss: 2.845\n","[ep 5][  100/  391] loss: 2.747\n","[ep 5][  200/  391] loss: 2.581\n","[ep 5][  300/  391] loss: 2.482\n","[ep 6][  100/  391] loss: 2.642\n","[ep 6][  200/  391] loss: 2.313\n","[ep 6][  300/  391] loss: 2.184\n","[ep 7][  100/  391] loss: 1.982\n","[ep 7][  200/  391] loss: 1.853\n","[ep 7][  300/  391] loss: 1.864\n","[ep 8][  100/  391] loss: 1.979\n","[ep 8][  200/  391] loss: 1.949\n","[ep 8][  300/  391] loss: 1.751\n","[ep 9][  100/  391] loss: 1.722\n","[ep 9][  200/  391] loss: 1.756\n","[ep 9][  300/  391] loss: 1.904\n","[ep 10][  100/  391] loss: 1.673\n","[ep 10][  200/  391] loss: 1.659\n","[ep 10][  300/  391] loss: 1.629\n","[ep 11][  100/  391] loss: 1.543\n","[ep 11][  200/  391] loss: 1.612\n","[ep 11][  300/  391] loss: 1.820\n","[ep 12][  100/  391] loss: 1.670\n","[ep 12][  200/  391] loss: 1.768\n","[ep 12][  300/  391] loss: 1.559\n","[ep 13][  100/  391] loss: 1.458\n","[ep 13][  200/  391] loss: 1.684\n","[ep 13][  300/  391] loss: 1.542\n","[ep 14][  100/  391] loss: 1.320\n","[ep 14][  200/  391] loss: 1.380\n","[ep 14][  300/  391] loss: 1.650\n","[ep 15][  100/  391] loss: 1.266\n","[ep 15][  200/  391] loss: 1.303\n","[ep 15][  300/  391] loss: 1.172\n","[ep 16][  100/  391] loss: 1.056\n","[ep 16][  200/  391] loss: 1.135\n","[ep 16][  300/  391] loss: 1.234\n","[ep 17][  100/  391] loss: 1.142\n","[ep 17][  200/  391] loss: 1.084\n","[ep 17][  300/  391] loss: 1.068\n","[ep 18][  100/  391] loss: 0.759\n","[ep 18][  200/  391] loss: 1.187\n","[ep 18][  300/  391] loss: 1.080\n","[ep 19][  100/  391] loss: 1.345\n","[ep 19][  200/  391] loss: 1.013\n","[ep 19][  300/  391] loss: 1.132\n","[ep 20][  100/  391] loss: 0.903\n","[ep 20][  200/  391] loss: 1.052\n","[ep 20][  300/  391] loss: 0.923\n","[ep 21][  100/  391] loss: 1.070\n","[ep 21][  200/  391] loss: 0.915\n","[ep 21][  300/  391] loss: 1.139\n","[ep 22][  100/  391] loss: 0.876\n","[ep 22][  200/  391] loss: 0.889\n","[ep 22][  300/  391] loss: 0.869\n","[ep 23][  100/  391] loss: 0.967\n","[ep 23][  200/  391] loss: 1.127\n","[ep 23][  300/  391] loss: 0.736\n","[ep 24][  100/  391] loss: 0.582\n","[ep 24][  200/  391] loss: 0.777\n","[ep 24][  300/  391] loss: 0.958\n","[ep 25][  100/  391] loss: 0.767\n","[ep 25][  200/  391] loss: 0.822\n","[ep 25][  300/  391] loss: 0.733\n","[ep 26][  100/  391] loss: 0.685\n","[ep 26][  200/  391] loss: 0.782\n","[ep 26][  300/  391] loss: 0.810\n","[ep 27][  100/  391] loss: 0.594\n","[ep 27][  200/  391] loss: 0.798\n","[ep 27][  300/  391] loss: 0.662\n","[ep 28][  100/  391] loss: 0.678\n","[ep 28][  200/  391] loss: 0.752\n","[ep 28][  300/  391] loss: 0.926\n","[ep 29][  100/  391] loss: 0.605\n","[ep 29][  200/  391] loss: 0.878\n","[ep 29][  300/  391] loss: 0.750\n","[ep 30][  100/  391] loss: 0.799\n","[ep 30][  200/  391] loss: 0.671\n","[ep 30][  300/  391] loss: 0.786\n","[ep 31][  100/  391] loss: 0.457\n","[ep 31][  200/  391] loss: 0.799\n","[ep 31][  300/  391] loss: 0.780\n","[ep 32][  100/  391] loss: 0.716\n","[ep 32][  200/  391] loss: 0.435\n","[ep 32][  300/  391] loss: 0.740\n","[ep 33][  100/  391] loss: 0.615\n","[ep 33][  200/  391] loss: 0.515\n","[ep 33][  300/  391] loss: 0.470\n","[ep 34][  100/  391] loss: 0.549\n","[ep 34][  200/  391] loss: 0.587\n","[ep 34][  300/  391] loss: 0.689\n","[ep 35][  100/  391] loss: 0.542\n","[ep 35][  200/  391] loss: 0.571\n","[ep 35][  300/  391] loss: 0.439\n","[ep 36][  100/  391] loss: 0.607\n","[ep 36][  200/  391] loss: 0.543\n","[ep 36][  300/  391] loss: 0.582\n","[ep 37][  100/  391] loss: 0.388\n","[ep 37][  200/  391] loss: 0.439\n","[ep 37][  300/  391] loss: 0.420\n","[ep 38][  100/  391] loss: 0.448\n","[ep 38][  200/  391] loss: 0.495\n","[ep 38][  300/  391] loss: 0.728\n","[ep 39][  100/  391] loss: 0.611\n","[ep 39][  200/  391] loss: 0.408\n","[ep 39][  300/  391] loss: 0.723\n","[ep 40][  100/  391] loss: 0.495\n","[ep 40][  200/  391] loss: 0.483\n","[ep 40][  300/  391] loss: 0.605\n","[ep 41][  100/  391] loss: 0.424\n","[ep 41][  200/  391] loss: 0.303\n","[ep 41][  300/  391] loss: 0.434\n","[ep 42][  100/  391] loss: 0.515\n","[ep 42][  200/  391] loss: 0.280\n","[ep 42][  300/  391] loss: 0.504\n","[ep 43][  100/  391] loss: 0.308\n","[ep 43][  200/  391] loss: 0.575\n","[ep 43][  300/  391] loss: 0.404\n","[ep 44][  100/  391] loss: 0.410\n","[ep 44][  200/  391] loss: 0.450\n","[ep 44][  300/  391] loss: 0.360\n","[ep 45][  100/  391] loss: 0.275\n","[ep 45][  200/  391] loss: 0.516\n","[ep 45][  300/  391] loss: 0.452\n","[ep 46][  100/  391] loss: 0.279\n","[ep 46][  200/  391] loss: 0.348\n","[ep 46][  300/  391] loss: 0.484\n","[ep 47][  100/  391] loss: 0.399\n","[ep 47][  200/  391] loss: 0.347\n","[ep 47][  300/  391] loss: 0.554\n","[ep 48][  100/  391] loss: 0.249\n","[ep 48][  200/  391] loss: 0.511\n","[ep 48][  300/  391] loss: 0.414\n","[ep 49][  100/  391] loss: 0.361\n","[ep 49][  200/  391] loss: 0.414\n","[ep 49][  300/  391] loss: 0.567\n","[ep 50][  100/  391] loss: 0.375\n","[ep 50][  200/  391] loss: 0.359\n","[ep 50][  300/  391] loss: 0.488\n","[ep 51][  100/  391] loss: 0.274\n","[ep 51][  200/  391] loss: 0.375\n","[ep 51][  300/  391] loss: 0.403\n","[ep 52][  100/  391] loss: 0.291\n","[ep 52][  200/  391] loss: 0.322\n","[ep 52][  300/  391] loss: 0.330\n","[ep 53][  100/  391] loss: 0.250\n","[ep 53][  200/  391] loss: 0.370\n","[ep 53][  300/  391] loss: 0.431\n","[ep 54][  100/  391] loss: 0.248\n","[ep 54][  200/  391] loss: 0.429\n","[ep 54][  300/  391] loss: 0.315\n","[ep 55][  100/  391] loss: 0.239\n","[ep 55][  200/  391] loss: 0.221\n","[ep 55][  300/  391] loss: 0.340\n","[ep 56][  100/  391] loss: 0.313\n","[ep 56][  200/  391] loss: 0.343\n","[ep 56][  300/  391] loss: 0.374\n","[ep 57][  100/  391] loss: 0.311\n","[ep 57][  200/  391] loss: 0.475\n","[ep 57][  300/  391] loss: 0.416\n","[ep 58][  100/  391] loss: 0.232\n","[ep 58][  200/  391] loss: 0.292\n","[ep 58][  300/  391] loss: 0.317\n","[ep 59][  100/  391] loss: 0.338\n","[ep 59][  200/  391] loss: 0.381\n","[ep 59][  300/  391] loss: 0.544\n","[ep 60][  100/  391] loss: 0.343\n","[ep 60][  200/  391] loss: 0.279\n","[ep 60][  300/  391] loss: 0.299\n","[ep 61][  100/  391] loss: 0.161\n","[ep 61][  200/  391] loss: 0.131\n","[ep 61][  300/  391] loss: 0.075\n","[ep 62][  100/  391] loss: 0.052\n","[ep 62][  200/  391] loss: 0.083\n","[ep 62][  300/  391] loss: 0.077\n","[ep 63][  100/  391] loss: 0.079\n","[ep 63][  200/  391] loss: 0.048\n","[ep 63][  300/  391] loss: 0.056\n","[ep 64][  100/  391] loss: 0.066\n","[ep 64][  200/  391] loss: 0.051\n","[ep 64][  300/  391] loss: 0.020\n","[ep 65][  100/  391] loss: 0.024\n","[ep 65][  200/  391] loss: 0.089\n","[ep 65][  300/  391] loss: 0.034\n","[ep 66][  100/  391] loss: 0.020\n","[ep 66][  200/  391] loss: 0.015\n","[ep 66][  300/  391] loss: 0.032\n","[ep 67][  100/  391] loss: 0.023\n","[ep 67][  200/  391] loss: 0.042\n","[ep 67][  300/  391] loss: 0.021\n","[ep 68][  100/  391] loss: 0.020\n","[ep 68][  200/  391] loss: 0.030\n","[ep 68][  300/  391] loss: 0.009\n","[ep 69][  100/  391] loss: 0.019\n","[ep 69][  200/  391] loss: 0.022\n","[ep 69][  300/  391] loss: 0.038\n","[ep 70][  100/  391] loss: 0.019\n","[ep 70][  200/  391] loss: 0.019\n","[ep 70][  300/  391] loss: 0.005\n","[ep 71][  100/  391] loss: 0.034\n","[ep 71][  200/  391] loss: 0.044\n","[ep 71][  300/  391] loss: 0.007\n","[ep 72][  100/  391] loss: 0.009\n","[ep 72][  200/  391] loss: 0.024\n","[ep 72][  300/  391] loss: 0.007\n","[ep 73][  100/  391] loss: 0.028\n","[ep 73][  200/  391] loss: 0.031\n","[ep 73][  300/  391] loss: 0.037\n","[ep 74][  100/  391] loss: 0.021\n","[ep 74][  200/  391] loss: 0.014\n","[ep 74][  300/  391] loss: 0.016\n","[ep 75][  100/  391] loss: 0.012\n","[ep 75][  200/  391] loss: 0.016\n","[ep 75][  300/  391] loss: 0.014\n","[ep 76][  100/  391] loss: 0.014\n","[ep 76][  200/  391] loss: 0.017\n","[ep 76][  300/  391] loss: 0.025\n","[ep 77][  100/  391] loss: 0.033\n","[ep 77][  200/  391] loss: 0.020\n","[ep 77][  300/  391] loss: 0.011\n","[ep 78][  100/  391] loss: 0.015\n","[ep 78][  200/  391] loss: 0.008\n","[ep 78][  300/  391] loss: 0.007\n","[ep 79][  100/  391] loss: 0.008\n","[ep 79][  200/  391] loss: 0.012\n","[ep 79][  300/  391] loss: 0.018\n","[ep 80][  100/  391] loss: 0.011\n","[ep 80][  200/  391] loss: 0.010\n","[ep 80][  300/  391] loss: 0.015\n","[ep 81][  100/  391] loss: 0.007\n","[ep 81][  200/  391] loss: 0.012\n","[ep 81][  300/  391] loss: 0.012\n","[ep 82][  100/  391] loss: 0.022\n","[ep 82][  200/  391] loss: 0.025\n","[ep 82][  300/  391] loss: 0.016\n","[ep 83][  100/  391] loss: 0.006\n","[ep 83][  200/  391] loss: 0.010\n","[ep 83][  300/  391] loss: 0.010\n","[ep 84][  100/  391] loss: 0.014\n","[ep 84][  200/  391] loss: 0.005\n","[ep 84][  300/  391] loss: 0.013\n","[ep 85][  100/  391] loss: 0.010\n","[ep 85][  200/  391] loss: 0.008\n","[ep 85][  300/  391] loss: 0.012\n","[ep 86][  100/  391] loss: 0.015\n","[ep 86][  200/  391] loss: 0.012\n","[ep 86][  300/  391] loss: 0.009\n","[ep 87][  100/  391] loss: 0.030\n","[ep 87][  200/  391] loss: 0.023\n","[ep 87][  300/  391] loss: 0.004\n","[ep 88][  100/  391] loss: 0.008\n","[ep 88][  200/  391] loss: 0.009\n","[ep 88][  300/  391] loss: 0.008\n","[ep 89][  100/  391] loss: 0.008\n","[ep 89][  200/  391] loss: 0.005\n","[ep 89][  300/  391] loss: 0.005\n","[ep 90][  100/  391] loss: 0.004\n","[ep 90][  200/  391] loss: 0.007\n","[ep 90][  300/  391] loss: 0.023\n","[ep 91][  100/  391] loss: 0.013\n","[ep 91][  200/  391] loss: 0.006\n","[ep 91][  300/  391] loss: 0.012\n","[ep 92][  100/  391] loss: 0.005\n","[ep 92][  200/  391] loss: 0.006\n","[ep 92][  300/  391] loss: 0.014\n","[ep 93][  100/  391] loss: 0.013\n","[ep 93][  200/  391] loss: 0.012\n","[ep 93][  300/  391] loss: 0.011\n","[ep 94][  100/  391] loss: 0.016\n","[ep 94][  200/  391] loss: 0.009\n","[ep 94][  300/  391] loss: 0.008\n","[ep 95][  100/  391] loss: 0.023\n","[ep 95][  200/  391] loss: 0.007\n","[ep 95][  300/  391] loss: 0.005\n","[ep 96][  100/  391] loss: 0.005\n","[ep 96][  200/  391] loss: 0.006\n","[ep 96][  300/  391] loss: 0.011\n","[ep 97][  100/  391] loss: 0.009\n","[ep 97][  200/  391] loss: 0.005\n","[ep 97][  300/  391] loss: 0.015\n","[ep 98][  100/  391] loss: 0.007\n","[ep 98][  200/  391] loss: 0.024\n","[ep 98][  300/  391] loss: 0.033\n","[ep 99][  100/  391] loss: 0.008\n","[ep 99][  200/  391] loss: 0.010\n","[ep 99][  300/  391] loss: 0.010\n","[ep 100][  100/  391] loss: 0.013\n","[ep 100][  200/  391] loss: 0.003\n","[ep 100][  300/  391] loss: 0.021\n","[ep 101][  100/  391] loss: 0.010\n","[ep 101][  200/  391] loss: 0.014\n","[ep 101][  300/  391] loss: 0.007\n","[ep 102][  100/  391] loss: 0.011\n","[ep 102][  200/  391] loss: 0.009\n","[ep 102][  300/  391] loss: 0.010\n","[ep 103][  100/  391] loss: 0.021\n","[ep 103][  200/  391] loss: 0.007\n","[ep 103][  300/  391] loss: 0.010\n","[ep 104][  100/  391] loss: 0.020\n","[ep 104][  200/  391] loss: 0.011\n","[ep 104][  300/  391] loss: 0.011\n","[ep 105][  100/  391] loss: 0.006\n","[ep 105][  200/  391] loss: 0.005\n","[ep 105][  300/  391] loss: 0.011\n","[ep 106][  100/  391] loss: 0.011\n","[ep 106][  200/  391] loss: 0.010\n","[ep 106][  300/  391] loss: 0.011\n","[ep 107][  100/  391] loss: 0.013\n","[ep 107][  200/  391] loss: 0.031\n","[ep 107][  300/  391] loss: 0.007\n","[ep 108][  100/  391] loss: 0.004\n","[ep 108][  200/  391] loss: 0.008\n","[ep 108][  300/  391] loss: 0.008\n","[ep 109][  100/  391] loss: 0.007\n","[ep 109][  200/  391] loss: 0.012\n","[ep 109][  300/  391] loss: 0.008\n","[ep 110][  100/  391] loss: 0.008\n","[ep 110][  200/  391] loss: 0.018\n","[ep 110][  300/  391] loss: 0.008\n","[ep 111][  100/  391] loss: 0.007\n","[ep 111][  200/  391] loss: 0.014\n","[ep 111][  300/  391] loss: 0.019\n","[ep 112][  100/  391] loss: 0.003\n","[ep 112][  200/  391] loss: 0.008\n","[ep 112][  300/  391] loss: 0.004\n","[ep 113][  100/  391] loss: 0.004\n","[ep 113][  200/  391] loss: 0.007\n","[ep 113][  300/  391] loss: 0.005\n","[ep 114][  100/  391] loss: 0.003\n","[ep 114][  200/  391] loss: 0.009\n","[ep 114][  300/  391] loss: 0.007\n","[ep 115][  100/  391] loss: 0.023\n","[ep 115][  200/  391] loss: 0.005\n","[ep 115][  300/  391] loss: 0.011\n","[ep 116][  100/  391] loss: 0.004\n","[ep 116][  200/  391] loss: 0.033\n","[ep 116][  300/  391] loss: 0.019\n","[ep 117][  100/  391] loss: 0.004\n","[ep 117][  200/  391] loss: 0.008\n","[ep 117][  300/  391] loss: 0.007\n","[ep 118][  100/  391] loss: 0.003\n","[ep 118][  200/  391] loss: 0.012\n","[ep 118][  300/  391] loss: 0.016\n","[ep 119][  100/  391] loss: 0.007\n","[ep 119][  200/  391] loss: 0.004\n","[ep 119][  300/  391] loss: 0.008\n","[ep 120][  100/  391] loss: 0.007\n","[ep 120][  200/  391] loss: 0.006\n","[ep 120][  300/  391] loss: 0.027\n","[ep 121][  100/  391] loss: 0.007\n","[ep 121][  200/  391] loss: 0.004\n","[ep 121][  300/  391] loss: 0.006\n","[ep 122][  100/  391] loss: 0.006\n","[ep 122][  200/  391] loss: 0.004\n","[ep 122][  300/  391] loss: 0.007\n","[ep 123][  100/  391] loss: 0.007\n","[ep 123][  200/  391] loss: 0.004\n","[ep 123][  300/  391] loss: 0.006\n","[ep 124][  100/  391] loss: 0.024\n","[ep 124][  200/  391] loss: 0.003\n","[ep 124][  300/  391] loss: 0.003\n","[ep 125][  100/  391] loss: 0.006\n","[ep 125][  200/  391] loss: 0.004\n","[ep 125][  300/  391] loss: 0.006\n","[ep 126][  100/  391] loss: 0.007\n","[ep 126][  200/  391] loss: 0.030\n","[ep 126][  300/  391] loss: 0.025\n","[ep 127][  100/  391] loss: 0.006\n","[ep 127][  200/  391] loss: 0.013\n","[ep 127][  300/  391] loss: 0.005\n","[ep 128][  100/  391] loss: 0.003\n","[ep 128][  200/  391] loss: 0.005\n","[ep 128][  300/  391] loss: 0.003\n","[ep 129][  100/  391] loss: 0.008\n","[ep 129][  200/  391] loss: 0.005\n","[ep 129][  300/  391] loss: 0.003\n","[ep 130][  100/  391] loss: 0.016\n","[ep 130][  200/  391] loss: 0.003\n","[ep 130][  300/  391] loss: 0.004\n","[ep 131][  100/  391] loss: 0.007\n","[ep 131][  200/  391] loss: 0.013\n","[ep 131][  300/  391] loss: 0.003\n","[ep 132][  100/  391] loss: 0.004\n","[ep 132][  200/  391] loss: 0.003\n","[ep 132][  300/  391] loss: 0.001\n","[ep 133][  100/  391] loss: 0.011\n","[ep 133][  200/  391] loss: 0.004\n","[ep 133][  300/  391] loss: 0.006\n","[ep 134][  100/  391] loss: 0.009\n","[ep 134][  200/  391] loss: 0.008\n","[ep 134][  300/  391] loss: 0.004\n","[ep 135][  100/  391] loss: 0.007\n","[ep 135][  200/  391] loss: 0.006\n","[ep 135][  300/  391] loss: 0.005\n","[ep 136][  100/  391] loss: 0.004\n","[ep 136][  200/  391] loss: 0.039\n","[ep 136][  300/  391] loss: 0.010\n","[ep 137][  100/  391] loss: 0.004\n","[ep 137][  200/  391] loss: 0.006\n","[ep 137][  300/  391] loss: 0.009\n","[ep 138][  100/  391] loss: 0.011\n","[ep 138][  200/  391] loss: 0.004\n","[ep 138][  300/  391] loss: 0.005\n","[ep 139][  100/  391] loss: 0.008\n","[ep 139][  200/  391] loss: 0.004\n","[ep 139][  300/  391] loss: 0.003\n","[ep 140][  100/  391] loss: 0.003\n","[ep 140][  200/  391] loss: 0.005\n","[ep 140][  300/  391] loss: 0.003\n","[ep 141][  100/  391] loss: 0.006\n","[ep 141][  200/  391] loss: 0.021\n","[ep 141][  300/  391] loss: 0.003\n","[ep 142][  100/  391] loss: 0.003\n","[ep 142][  200/  391] loss: 0.005\n","[ep 142][  300/  391] loss: 0.008\n","[ep 143][  100/  391] loss: 0.005\n","[ep 143][  200/  391] loss: 0.002\n","[ep 143][  300/  391] loss: 0.008\n","[ep 144][  100/  391] loss: 0.004\n","[ep 144][  200/  391] loss: 0.004\n","[ep 144][  300/  391] loss: 0.003\n","[ep 145][  100/  391] loss: 0.010\n","[ep 145][  200/  391] loss: 0.004\n","[ep 145][  300/  391] loss: 0.004\n","[ep 146][  100/  391] loss: 0.004\n","[ep 146][  200/  391] loss: 0.007\n","[ep 146][  300/  391] loss: 0.004\n","[ep 147][  100/  391] loss: 0.006\n","[ep 147][  200/  391] loss: 0.010\n","[ep 147][  300/  391] loss: 0.017\n","[ep 148][  100/  391] loss: 0.004\n","[ep 148][  200/  391] loss: 0.004\n","[ep 148][  300/  391] loss: 0.007\n","[ep 149][  100/  391] loss: 0.003\n","[ep 149][  200/  391] loss: 0.011\n","[ep 149][  300/  391] loss: 0.013\n","[ep 150][  100/  391] loss: 0.004\n","[ep 150][  200/  391] loss: 0.004\n","[ep 150][  300/  391] loss: 0.004\n","[ep 151][  100/  391] loss: 0.002\n","[ep 151][  200/  391] loss: 0.006\n","[ep 151][  300/  391] loss: 0.003\n","[ep 152][  100/  391] loss: 0.010\n","[ep 152][  200/  391] loss: 0.006\n","[ep 152][  300/  391] loss: 0.005\n","[ep 153][  100/  391] loss: 0.008\n","[ep 153][  200/  391] loss: 0.005\n","[ep 153][  300/  391] loss: 0.004\n","[ep 154][  100/  391] loss: 0.003\n","[ep 154][  200/  391] loss: 0.005\n","[ep 154][  300/  391] loss: 0.006\n","[ep 155][  100/  391] loss: 0.006\n","[ep 155][  200/  391] loss: 0.003\n","[ep 155][  300/  391] loss: 0.004\n","[ep 156][  100/  391] loss: 0.003\n","[ep 156][  200/  391] loss: 0.008\n","[ep 156][  300/  391] loss: 0.006\n","[ep 157][  100/  391] loss: 0.010\n","[ep 157][  200/  391] loss: 0.007\n","[ep 157][  300/  391] loss: 0.009\n","[ep 158][  100/  391] loss: 0.004\n","[ep 158][  200/  391] loss: 0.004\n","[ep 158][  300/  391] loss: 0.004\n","[ep 159][  100/  391] loss: 0.003\n","[ep 159][  200/  391] loss: 0.005\n","[ep 159][  300/  391] loss: 0.005\n","[ep 160][  100/  391] loss: 0.005\n","[ep 160][  200/  391] loss: 0.005\n","[ep 160][  300/  391] loss: 0.005\n","[ep 161][  100/  391] loss: 0.004\n","[ep 161][  200/  391] loss: 0.012\n","[ep 161][  300/  391] loss: 0.003\n","[ep 162][  100/  391] loss: 0.004\n","[ep 162][  200/  391] loss: 0.006\n","[ep 162][  300/  391] loss: 0.004\n","[ep 163][  100/  391] loss: 0.004\n","[ep 163][  200/  391] loss: 0.003\n","[ep 163][  300/  391] loss: 0.004\n","[ep 164][  100/  391] loss: 0.005\n","[ep 164][  200/  391] loss: 0.003\n","[ep 164][  300/  391] loss: 0.005\n","[ep 165][  100/  391] loss: 0.005\n","[ep 165][  200/  391] loss: 0.003\n","[ep 165][  300/  391] loss: 0.002\n","[ep 166][  100/  391] loss: 0.003\n","[ep 166][  200/  391] loss: 0.003\n","[ep 166][  300/  391] loss: 0.004\n","[ep 167][  100/  391] loss: 0.003\n","[ep 167][  200/  391] loss: 0.003\n","[ep 167][  300/  391] loss: 0.001\n","[ep 168][  100/  391] loss: 0.005\n","[ep 168][  200/  391] loss: 0.002\n","[ep 168][  300/  391] loss: 0.004\n","[ep 169][  100/  391] loss: 0.006\n","[ep 169][  200/  391] loss: 0.011\n","[ep 169][  300/  391] loss: 0.012\n","[ep 170][  100/  391] loss: 0.002\n","[ep 170][  200/  391] loss: 0.003\n","[ep 170][  300/  391] loss: 0.003\n","[ep 171][  100/  391] loss: 0.004\n","[ep 171][  200/  391] loss: 0.003\n","[ep 171][  300/  391] loss: 0.004\n","[ep 172][  100/  391] loss: 0.005\n","[ep 172][  200/  391] loss: 0.005\n","[ep 172][  300/  391] loss: 0.005\n","[ep 173][  100/  391] loss: 0.006\n","[ep 173][  200/  391] loss: 0.004\n","[ep 173][  300/  391] loss: 0.004\n","[ep 174][  100/  391] loss: 0.004\n","[ep 174][  200/  391] loss: 0.004\n","[ep 174][  300/  391] loss: 0.004\n","[ep 175][  100/  391] loss: 0.004\n","[ep 175][  200/  391] loss: 0.008\n","[ep 175][  300/  391] loss: 0.003\n","[ep 176][  100/  391] loss: 0.005\n","[ep 176][  200/  391] loss: 0.003\n","[ep 176][  300/  391] loss: 0.003\n","[ep 177][  100/  391] loss: 0.005\n","[ep 177][  200/  391] loss: 0.003\n","[ep 177][  300/  391] loss: 0.004\n","[ep 178][  100/  391] loss: 0.006\n","[ep 178][  200/  391] loss: 0.004\n","[ep 178][  300/  391] loss: 0.002\n","[ep 179][  100/  391] loss: 0.004\n","[ep 179][  200/  391] loss: 0.003\n","[ep 179][  300/  391] loss: 0.005\n","[ep 180][  100/  391] loss: 0.003\n","[ep 180][  200/  391] loss: 0.004\n","[ep 180][  300/  391] loss: 0.008\n","[ep 181][  100/  391] loss: 0.004\n","[ep 181][  200/  391] loss: 0.003\n","[ep 181][  300/  391] loss: 0.005\n","[ep 182][  100/  391] loss: 0.004\n","[ep 182][  200/  391] loss: 0.009\n","[ep 182][  300/  391] loss: 0.002\n","[ep 183][  100/  391] loss: 0.003\n","[ep 183][  200/  391] loss: 0.005\n","[ep 183][  300/  391] loss: 0.004\n","[ep 184][  100/  391] loss: 0.004\n","[ep 184][  200/  391] loss: 0.003\n","[ep 184][  300/  391] loss: 0.002\n","[ep 185][  100/  391] loss: 0.003\n","[ep 185][  200/  391] loss: 0.003\n","[ep 185][  300/  391] loss: 0.003\n","[ep 186][  100/  391] loss: 0.004\n","[ep 186][  200/  391] loss: 0.013\n","[ep 186][  300/  391] loss: 0.002\n","[ep 187][  100/  391] loss: 0.005\n","[ep 187][  200/  391] loss: 0.006\n","[ep 187][  300/  391] loss: 0.004\n","[ep 188][  100/  391] loss: 0.014\n","[ep 188][  200/  391] loss: 0.003\n","[ep 188][  300/  391] loss: 0.006\n","[ep 189][  100/  391] loss: 0.003\n","[ep 189][  200/  391] loss: 0.009\n","[ep 189][  300/  391] loss: 0.013\n","[ep 190][  100/  391] loss: 0.003\n","[ep 190][  200/  391] loss: 0.004\n","[ep 190][  300/  391] loss: 0.003\n","[ep 191][  100/  391] loss: 0.005\n","[ep 191][  200/  391] loss: 0.003\n","[ep 191][  300/  391] loss: 0.005\n","[ep 192][  100/  391] loss: 0.004\n","[ep 192][  200/  391] loss: 0.005\n","[ep 192][  300/  391] loss: 0.005\n","[ep 193][  100/  391] loss: 0.009\n","[ep 193][  200/  391] loss: 0.003\n","[ep 193][  300/  391] loss: 0.003\n","[ep 194][  100/  391] loss: 0.006\n","[ep 194][  200/  391] loss: 0.014\n","[ep 194][  300/  391] loss: 0.004\n","[ep 195][  100/  391] loss: 0.007\n","[ep 195][  200/  391] loss: 0.003\n","[ep 195][  300/  391] loss: 0.003\n","[ep 196][  100/  391] loss: 0.006\n","[ep 196][  200/  391] loss: 0.006\n","[ep 196][  300/  391] loss: 0.003\n","[ep 197][  100/  391] loss: 0.003\n","[ep 197][  200/  391] loss: 0.006\n","[ep 197][  300/  391] loss: 0.007\n","[ep 198][  100/  391] loss: 0.006\n","[ep 198][  200/  391] loss: 0.011\n","[ep 198][  300/  391] loss: 0.006\n","[ep 199][  100/  391] loss: 0.013\n","[ep 199][  200/  391] loss: 0.003\n","[ep 199][  300/  391] loss: 0.002\n","[ep 200][  100/  391] loss: 0.004\n","[ep 200][  200/  391] loss: 0.012\n","[ep 200][  300/  391] loss: 0.003\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PRCaFaSEyPP","outputId":"da35e4b4-1231-48b2-c6c9-ded2f8c6728c","executionInfo":{"status":"ok","timestamp":1649324367397,"user_tz":-480,"elapsed":9087,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 10000 test images: 37.45 %\n"]}],"source":["# load trained model\n","# model = torch.load(\"./model.pth\")\n","# model.to(device)\n","\n","# fixed testing process\n","correct = 0\n","total = 0\n","# since we're not training, we don't need to calculate the gradients for our outputs\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        # calculate outputs by running images through the network\n","        outputs = model(images)\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')"]},{"cell_type":"markdown","source":["Accuracy of the network on the 10000 test images: 60.19 %"],"metadata":{"id":"nJ-vWfp5pcE2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClqMY-sxSjSt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4956c58e-208f-43c7-d24d-5fc009778ce5","executionInfo":{"status":"ok","timestamp":1649090135633,"user_tz":-480,"elapsed":360,"user":{"displayName":"邱柏鎧","userId":"04478619653307828795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["du: cannot access 'model.pth': No such file or directory\n"]}],"source":["# model = models.mobilenet_v3_large()\n","# torch.save(model, \"./model.pth\")\n","\n","# see size of saved model\n","! du -h model.pth"]},{"cell_type":"code","source":[""],"metadata":{"id":"TGvw3Nxya-QG"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"SEResNet.ipynb","provenance":[{"file_id":"1MjKxgS8L92nlBRAAMMHBl3EQegCPCpT5","timestamp":1649257617305},{"file_id":"16dN4wEownA_cQTPbeaa6aztk6oCCHo5v","timestamp":1649253967507}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"95f99430742048179038a67ae16aa8ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6ad0fb4b51c4b0a920f0b87f6c64994","IPY_MODEL_43a7e279c4bf4157bbd7ea02f175e32d","IPY_MODEL_3f76ce15e5e94099a43be709496bf0b4"],"layout":"IPY_MODEL_7eee45cf04c54715ab15d2880890e1d3"}},"a6ad0fb4b51c4b0a920f0b87f6c64994":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9beefc82a2434ef0afb808bdcac558a2","placeholder":"​","style":"IPY_MODEL_61a6b7f8c61e4f629f39d431ccb403f5","value":""}},"43a7e279c4bf4157bbd7ea02f175e32d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a3088e21964c3983175488618af6ed","max":169001437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f96dc32239a4a48aef129770c4d56aa","value":169001437}},"3f76ce15e5e94099a43be709496bf0b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70b3241700ad4c788c1b6cab40340dcc","placeholder":"​","style":"IPY_MODEL_452f457cc4c34285aef6c6f5620cd0df","value":" 169001984/? [00:02&lt;00:00, 61607416.21it/s]"}},"7eee45cf04c54715ab15d2880890e1d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9beefc82a2434ef0afb808bdcac558a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61a6b7f8c61e4f629f39d431ccb403f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4a3088e21964c3983175488618af6ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f96dc32239a4a48aef129770c4d56aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70b3241700ad4c788c1b6cab40340dcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"452f457cc4c34285aef6c6f5620cd0df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}